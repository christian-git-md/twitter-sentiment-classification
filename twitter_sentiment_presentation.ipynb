{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5dedb85-382a-4e1f-8dd4-e4f1c7d66719",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Datenanalyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f6a7c0f-a8ad-46a8-a9af-6e0d1a94b589",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.dataset import load_tweet_sentiment_csv_file\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9e5b9fc-1a96-40d5-84a0-299c7338ca0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3364</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>I mentioned on Facebook that I was struggling for motivation to go for a run the other day, whic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>352</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>BBC News - Amazon boss Jeff Bezos rejects claims company acted like a 'drug dealer' bbc.co.uk/ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8312</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Negative</td>\n",
       "      <td>@Microsoft Why do I pay for WORD when it functions so poorly on my @SamsungUS Chromebook? üôÑ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4371</td>\n",
       "      <td>CS-GO</td>\n",
       "      <td>Negative</td>\n",
       "      <td>CSGO matchmaking is so full of closet hacking, it's a truly awful game.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4433</td>\n",
       "      <td>Google</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Now the President is slapping Americans in the face that he really did commit an unlawful act af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6273</td>\n",
       "      <td>FIFA</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hi @EAHelp I‚Äôve had Madeleine McCann in my cellar for the past 13 years and the little sneaky th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7925</td>\n",
       "      <td>MaddenNFL</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Thank you @EAMaddenNFL!! \\n\\nNew TE Austin Hooper in the ORANGE &amp; BROWN!! \\n\\n#Browns | @AustinH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id        tag   sentiment  \\\n",
       "0      3364   Facebook  Irrelevant   \n",
       "1       352     Amazon     Neutral   \n",
       "2      8312  Microsoft    Negative   \n",
       "3      4371      CS-GO    Negative   \n",
       "4      4433     Google     Neutral   \n",
       "5      6273       FIFA    Negative   \n",
       "6      7925  MaddenNFL    Positive   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  I mentioned on Facebook that I was struggling for motivation to go for a run the other day, whic...  \n",
       "1  BBC News - Amazon boss Jeff Bezos rejects claims company acted like a 'drug dealer' bbc.co.uk/ne...  \n",
       "2          @Microsoft Why do I pay for WORD when it functions so poorly on my @SamsungUS Chromebook? üôÑ  \n",
       "3                              CSGO matchmaking is so full of closet hacking, it's a truly awful game.  \n",
       "4  Now the President is slapping Americans in the face that he really did commit an unlawful act af...  \n",
       "5  Hi @EAHelp I‚Äôve had Madeleine McCann in my cellar for the past 13 years and the little sneaky th...  \n",
       "6  Thank you @EAMaddenNFL!! \\n\\nNew TE Austin Hooper in the ORANGE & BROWN!! \\n\\n#Browns | @AustinH...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_path = \"data/training.csv\"\n",
    "train_df = load_tweet_sentiment_csv_file(train_file_path)\n",
    "val_file_path = \"data/validation.csv\"\n",
    "val_df = load_tweet_sentiment_csv_file(val_file_path)\n",
    "val_df[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f1fee0-b79e-43b9-837a-8ce650650e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74682, 4)\n",
      "['Positive' 'Neutral' 'Negative' 'Irrelevant']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(train_df[\"sentiment\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20fbf7ed-29d7-476e-a99e-b0b321635fb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Negative      22542\n",
       "Positive      20832\n",
       "Neutral       18318\n",
       "Irrelevant    12990\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"sentiment\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfb579e-5105-4c71-a82e-0d517cd9df24",
   "metadata": {
    "tags": []
   },
   "source": [
    "Trainings und Validierungsdatensatz sind leicht unausgeglichen. Es gibt neben Positive und Negative zus√§tzliche Klassen Neutral und Irrelevant was untypisch ist und von vortrainierten Referenzmodellen nicht abgedeckt ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6975e5-10a0-4f8a-8721-3691fc613601",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tag</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>2411</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>2496</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>2503</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>2532</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>2595</td>\n",
       "      <td>Borderlands</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73972</th>\n",
       "      <td>9073</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73973</th>\n",
       "      <td>9073</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74421</th>\n",
       "      <td>9154</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74422</th>\n",
       "      <td>9154</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74423</th>\n",
       "      <td>9154</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>686 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       tweet_id          tag sentiment text\n",
       "61         2411  Borderlands   Neutral  NaN\n",
       "553        2496  Borderlands   Neutral  NaN\n",
       "589        2503  Borderlands   Neutral  NaN\n",
       "745        2532  Borderlands  Positive  NaN\n",
       "1105       2595  Borderlands  Positive  NaN\n",
       "...         ...          ...       ...  ...\n",
       "73972      9073       Nvidia  Positive  NaN\n",
       "73973      9073       Nvidia  Positive  NaN\n",
       "74421      9154       Nvidia  Positive  NaN\n",
       "74422      9154       Nvidia  Positive  NaN\n",
       "74423      9154       Nvidia  Positive  NaN\n",
       "\n",
       "[686 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[train_df[\"text\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eab03fcc-d48b-412e-a16d-746d3d4c8747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that was the first borderlands session in a long time where i actually had a really satisfying combat experience. i got some really good kills\n",
      "this was the first Borderlands session in a long time where i actually had a really satisfying fighting experience. i got some really good kills\n",
      "that was the first borderlands session in a long time where i actually had a really satisfying combat experience. i got some really good kills\n",
      "that was the first borderlands session in a long time where i actually enjoyed a really satisfying combat experience. i got some rather good kills\n",
      "that I was the first real borderlands session in a nice long wait time where i actually had a really satisfying combat experience. and i got some really good kills\n",
      "that was the first borderlands session in a hot row where i actually had a really bad combat experience. i did some really good kills\n"
     ]
    }
   ],
   "source": [
    "for row in train_df[train_df[\"tweet_id\"]==2404][\"text\"]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907b967f-940e-4382-bfce-e0f1a7203169",
   "metadata": {},
   "source": [
    "Der Trainingsdatensatz beinhaltet zus√§tzliche augmentierte Samples, die grammatikalisch inkorrekt oder leer sein k√∂nnen. Es scheint dass, das erste Sample f√ºr jede tweet_id die Orginaldaten beeinh√§lt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555346e5-7a83-4d03-b2a1-9c725d121356",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12447"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"tweet_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476dbf2-3959-46c1-bd99-d8d7bd9b3413",
   "metadata": {
    "tags": []
   },
   "source": [
    "Der Trainingsdatensatz beeinhaltet 12447 samples von unique tweets, was f√ºr ein sinnvolles finetuning ausreichen sollte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ae67319-8cf6-451a-844d-c604d5304d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df[\"tweet_id\"].apply(lambda x: x in train_df[\"tweet_id\"]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09be7b-3e34-44d7-8b05-4e7a86a97cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "Alle Tweets aus dem Validierungsdatensatz kommen auch im Trainingsdatensatz vor. Dieser Leak sollte behoben werden. Da die tweet_ids gegeben sind, k√∂nnen wir die Validierungssamples direkt aus dem Trainingsdatensatz l√∂schen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5821f985-cfab-42d1-8ec1-481c26f84515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_train</th>\n",
       "      <th>text_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Wine drunk playing the new Borderlands . . . Goddess life is a fun life.. . .  findom</td>\n",
       "      <td>Wine drunk playing the new Borderlands üò©\\n\\nGoddess life is a fun life.\\n\\n‚ú® findom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>I love u guys</td>\n",
       "      <td>I love u guys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Atleast I have Borderlands to cheer me up :(</td>\n",
       "      <td>Atleast I have Borderlands to cheer me up :(</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Chris loves me in borderlands one and two.</td>\n",
       "      <td>Chris loves me in borderlands one and two.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>This cricket has been the worst hivemind of fandom I have done this more times than I would love...</td>\n",
       "      <td>This cricket has been the worst hivemind of fandom I have done this more times than I would love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5 games, 5 Mutuals. . Pok√©mon. Borderlands (p much all). Sims 4 (haven't played in forever tho)....</td>\n",
       "      <td>5 games, 5 Mutuals\\n\\nPok√©mon\\nBorderlands (p much all)\\nSims 4 (haven't played in forever tho)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I want to thank</td>\n",
       "      <td>I want to thank #SSKYWILDKATSSS for letting me run the new Borderlands 3 DLC with him last night...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>So after the past 9 days of streaming on the bounce, and last nights insanely brilliant session ...</td>\n",
       "      <td>So after the past 9 days of streaming on the bounce, and last nights insanely brilliant session ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Today sucked so it‚Äôs time to drink wine n play borderlands until the sun comes up so I can hate ...</td>\n",
       "      <td>Today sucked so it‚Äôs time to drink wine n play borderlands until the sun comes up so I can hate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Shitting around | Borderlands 3 | Part 5.5 twitch.tv/slayer3000bot</td>\n",
       "      <td>Shitting around | Borderlands 3 | Part 5.5 twitch.tv/slayer3000bot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              text_train  \\\n",
       "60                 Wine drunk playing the new Borderlands . . . Goddess life is a fun life.. . .  findom   \n",
       "66                                                                                         I love u guys   \n",
       "72                                                          Atleast I have Borderlands to cheer me up :(   \n",
       "78                                                            Chris loves me in borderlands one and two.   \n",
       "84   This cricket has been the worst hivemind of fandom I have done this more times than I would love...   \n",
       "90   5 games, 5 Mutuals. . Pok√©mon. Borderlands (p much all). Sims 4 (haven't played in forever tho)....   \n",
       "96                                                                                     I want to thank     \n",
       "102  So after the past 9 days of streaming on the bounce, and last nights insanely brilliant session ...   \n",
       "108  Today sucked so it‚Äôs time to drink wine n play borderlands until the sun comes up so I can hate ...   \n",
       "114                                   Shitting around | Borderlands 3 | Part 5.5 twitch.tv/slayer3000bot   \n",
       "\n",
       "                                                                                                text_val  \n",
       "60                   Wine drunk playing the new Borderlands üò©\\n\\nGoddess life is a fun life.\\n\\n‚ú® findom  \n",
       "66                                                                                         I love u guys  \n",
       "72                                                          Atleast I have Borderlands to cheer me up :(  \n",
       "78                                                            Chris loves me in borderlands one and two.  \n",
       "84   This cricket has been the worst hivemind of fandom I have done this more times than I would love...  \n",
       "90   5 games, 5 Mutuals\\n\\nPok√©mon\\nBorderlands (p much all)\\nSims 4 (haven't played in forever tho)\\...  \n",
       "96   I want to thank #SSKYWILDKATSSS for letting me run the new Borderlands 3 DLC with him last night...  \n",
       "102  So after the past 9 days of streaming on the bounce, and last nights insanely brilliant session ...  \n",
       "108  Today sucked so it‚Äôs time to drink wine n play borderlands until the sun comes up so I can hate ...  \n",
       "114                                   Shitting around | Borderlands 3 | Part 5.5 twitch.tv/slayer3000bot  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "merged_df = pd.merge(train_df, val_df, on='tweet_id', suffixes=('_train', '_val'))\n",
    "merged_df = merged_df[~merged_df.duplicated(subset=\"tweet_id\", keep=\"first\")]\n",
    "merged_df[[\"text_train\", \"text_val\"]][10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbdae7-8c17-458a-a952-21046df56175",
   "metadata": {},
   "source": [
    "Wenn wir die Samples mit √ºbereinstimmender tweet_id aus dem Validierungsdatensatz mit dem Trainingsdatensatz vergleichen, k√∂nnen wir ausserdem ein paar zus√§tzliche Modifikationen des Traingsdatensatzes sehen. Ich konnte die folgenden finden:\n",
    "- Zeilenumbr√ºche entfernt\n",
    "- Manche Emojis entfernt\n",
    "- Die meisten Hashtags entfernt\n",
    "- Manche Links entfernt\n",
    "\n",
    "Wir sollten vor der Evaluierung die Modifikationen auch auf die Validierungsdaten anwenden, damit der Modell Input besser mit dem Training √ºbereinstimmt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f12615-89f9-4bbc-9dc3-beacbae22d7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Datenaufbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a919360-f405-44c6-95ad-2880193312da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load model/dataset.py\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "\n",
    "def load_tweet_sentiment_csv_file(file_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a sentiment file for the coding challenge from the disk and add some fitting column names.\n",
    "    \"\"\"\n",
    "    column_names = [\"tweet_id\", \"tag\", \"sentiment\", \"text\"]\n",
    "    return pd.read_csv(file_name, header=None, names=column_names)\n",
    "\n",
    "\n",
    "def prepare_text_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Does some basic cleaning on the data: removes invalid or too short samples and unwanted text\n",
    "    segments.\n",
    "    \"\"\"\n",
    "    df = df[~df[\"text\"].isna()]\n",
    "    df = df[df[\"text\"].str.count(\"[a-zA-Z]\") >= 5]\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda text: re.sub(r\"\\n\", \"\", text)) # \\n only appears in validation data\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda text: re.sub(r\"#\\w+\", \"\", text)) # Remove hashtags\n",
    "    df[\"text\"].apply(lambda text: re.sub(r\"http\\S+|www\\.\\S+\", \"\", text))  # Remove URLs\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_labels(df: pd.DataFrame, label_map: Dict[str, int]):\n",
    "    \"\"\"Creates the column for the labels, created by a map from the \"sentiment\" entry.\"\"\"\n",
    "    df[\"label\"] = [label_map[label_name] for label_name in df[\"sentiment\"]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_leaked_training_samples(train_df: pd.DataFrame, val_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes the samples that appear in both training and validation data from the training data.\"\"\"\n",
    "    return train_df[~train_df[\"tweet_id\"].isin(val_df[\"tweet_id\"])]\n",
    "\n",
    "\n",
    "def remove_augmented_training_samples(train_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Removes all augmented versions of a training sample, except the first one.\"\"\"\n",
    "    train_df = train_df[~train_df.duplicated(subset=\"tweet_id\", keep=\"first\")]\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def create_datasets(\n",
    "    train_file_name: str,\n",
    "    val_file_name: str,\n",
    "    label_definitions: Dict[str, int],\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Loads the string data from the disk, cleans and preprocesses the entries, converts to a Dataset and applies the\n",
    "    given tokenizer.\n",
    "    \"\"\"\n",
    "    train_df = load_tweet_sentiment_csv_file(train_file_name)\n",
    "    val_df = load_tweet_sentiment_csv_file(val_file_name)\n",
    "\n",
    "    train_df = remove_leaked_training_samples(train_df, val_df)\n",
    "    train_df = remove_augmented_training_samples(train_df)\n",
    "\n",
    "    train_df = prepare_text_data(train_df)\n",
    "    val_df = prepare_text_data(val_df)\n",
    "\n",
    "    train_df = prepare_labels(train_df, label_definitions)\n",
    "    val_df = prepare_labels(val_df, label_definitions)\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
    "    val_dataset = Dataset.from_pandas(val_df[[\"text\", \"label\"]])\n",
    "\n",
    "    tokenizer_fn = lambda examples: tokenizer(examples[\"text\"], truncation=True)\n",
    "    tokenized_train = train_dataset.map(tokenizer_fn, batched=True)\n",
    "    tokenized_test = val_dataset.map(tokenizer_fn, batched=True)\n",
    "    return tokenized_train, tokenized_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d991192-ccde-47c2-952d-0e19b858ab5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Bevor wir den Trainingsdatensatz erstellen, bereinigen wir ihn um die oben festgestellten Probleme zu beheben. Bei den augmentierten Samples habe ich beide Versionen, mit und ohne Augmentationen getestet (siehe weiter unten).\n",
    "\n",
    "Ich habe hier auf zus√§tzliche Modifikationen des Text Inputs (wie l√∂schen von extra Leerzeichen, ersetzen von Spezialzeichen) verzichtet, da der Bert Tokenizer den wir verwenden solche F√§lle sinnvoll verarbeiten kann und damit das \"alignment\" der Daten mit dem vortrainierten Modell h√∂her ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c762d954-6a1e-4704-a05a-e3ad44b688de",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911b1db-7cb4-473d-8e50-94887ea9ebf6",
   "metadata": {},
   "source": [
    "Ich m√∂chte ein vortrainiertes Modell verwenden und mit den 4 gegebenen Klassen finetunen. Ein vortrainiertes NLP model sollte generelle aus Konzepte der Sprache erlernt haben um den Satz zu evaluieren. Das Finetuning zielt darauf ab, das Modell auf die genaue Domain, Sentimentdaten von Twitter, zu spezialisieren.\n",
    "\n",
    "Das basis Model ist eine kleinere \"distillierte\" aber kompetente Version des generellen Sprachmodells Bert, welches auf Huggingface verf√ºgbar ist. Ich verwende auch die Huggingface Library um das Modell zu fine-tunen. Ich habe kein Hyperparemeter-Tuning durchgef√ºhrt und verwende die von Huggingface empfohƒ∫enen Hyperparemeter f√ºr Transfer Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4cfd38-e2ae-4e2a-8653-2f81a1f0c5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load model/trainer\n",
    "from model.config import train_file_name, val_file_name, label_definitions, output_dir\n",
    "from model.dataset import create_datasets\n",
    "from model.metrics import compute_metrics\n",
    "from model.plots import plot_confusion_matrix\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Main training pipeline, load and pre-process the data, initialize the trainer from a pretrained model and start\n",
    "    the training loop.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    train_dataset, val_dataset = create_datasets(train_file_name, val_file_name, label_definitions, tokenizer)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    id2label = {id: label for label, id in label_definitions.items()}\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\",\n",
    "        num_labels=4,\n",
    "        id2label=id2label,\n",
    "        label2id=label_definitions,\n",
    "        ignore_mismatched_sizes=True,\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        save_strategy=\"epoch\",\n",
    "        push_to_hub=False,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        eval_steps=1,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5d9988-07f7-4a0d-90ed-eab3bb4db651",
   "metadata": {
    "tags": []
   },
   "source": [
    "  # Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9af6db7-8dc0-4202-9862-5430803becdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/pipelines/text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "/twitter_sentiment/model/metrics.py:36: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  load_accuracy = load_metric(\"accuracy\")\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/pipelines/text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/pipelines/text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/transformers/pipelines/text_classification.py:107: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>binary_accuracy</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distilbert-base-uncased-finetuned-sst-2-english</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distilbert-base-uncased-finetuned-twitter-noleak</td>\n",
       "      <td>0.647</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distilbert-base-uncased-finetuned-twitter-noleak-noduplicates</td>\n",
       "      <td>0.641</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distilbert-base-uncased-finetuned-twitter-leak</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      model_name  accuracy  \\\n",
       "0                distilbert-base-uncased-finetuned-sst-2-english     0.438   \n",
       "1               distilbert-base-uncased-finetuned-twitter-noleak     0.647   \n",
       "2  distilbert-base-uncased-finetuned-twitter-noleak-noduplicates     0.641   \n",
       "3                 distilbert-base-uncased-finetuned-twitter-leak     0.972   \n",
       "\n",
       "   binary_accuracy     f1  \n",
       "0            0.807  0.285  \n",
       "1            0.862  0.632  \n",
       "2            0.860  0.628  \n",
       "3            0.991  0.971  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.evaluation import eval_model_summary\n",
    "\n",
    "model_names = [\n",
    "    \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    \"christian-git-md/distilbert-base-uncased-finetuned-twitter-noleak\",\n",
    "    \"christian-git-md/distilbert-base-uncased-finetuned-twitter-noleak-noduplicates\",\n",
    "    \"christian-git-md/distilbert-base-uncased-finetuned-twitter-leak\",\n",
    "]\n",
    "eval_model_summary(model_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a11a77-361c-4161-ae17-b104e8cad7d3",
   "metadata": {},
   "source": [
    "Die Modelle die trainiert und evaluiert wurden sind:\n",
    "- <b>distilbert-base-uncased-finetuned-sst-2-english:</b> Ein generelles Sentiment Modell, das als Referenz evaluiert wurde. Da dieses Model nur auf zwei Klassen trainiert wurde ist hier nur die \"binary_accuracy\", also die Accuracy auf positive / negative Samples relevant\n",
    "- <b>distilbert-base-uncased-finetuned-twitter-noleak:</b> Das trainierte Modell wie oben beschrieben, mit allen augmentierten Samples\n",
    "- <b>distilbert-base-uncased-finetuned-twitter-noleak-noduplicates:</b> Mit training ohne augmentierten Samples\n",
    "- <b>distilbert-base-uncased-finetuned-twitter-leak:</b> Trainining ohne Entfernung der Validierungssamples aus den Trainingsdaten. Wie zu erwarten resultiert dies in eine unrealistisch hohe Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660c562-d614-4434-80af-e33eb360671e",
   "metadata": {},
   "source": [
    "# Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af367c0-8d36-4381-bb27-fb4ff6090ebc",
   "metadata": {},
   "source": [
    "Ein Dockerfile f√ºr den Container der das Modell per fastapi App auf dem Port 9090 bereitstellt. \n",
    "\n",
    "Die App implementiert neben den Endpoints nur eine einfache Queue um Text aus Requests sequenziell von dem Modell evaluieren zu lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2d71f-6306-4cac-bac3-6a50ad92c934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load docker/Dockerfile\n",
    "FROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\n",
    "RUN pip install 'transformers[torch]' \\\n",
    " fastapi \\\n",
    " uvicorn \\\n",
    " python-multipart \\\n",
    " scikit-learn \\\n",
    " emoji \\\n",
    " datasets \\\n",
    " huggingface_hub \\\n",
    " evaluate \\\n",
    " matplotlib \\\n",
    " jupyterlab\n",
    "RUN apt-get update\n",
    "RUN apt-get install -y git-lfs\n",
    "COPY download_models.py .\n",
    "RUN python3 download_models.py\n",
    "COPY entrypoint.sh /usr/src/app/entrypoint.sh\n",
    "RUN chmod +x /usr/src/app/entrypoint.sh\n",
    "ENTRYPOINT [\"/usr/src/app/entrypoint.sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d13cda-a4d4-4e2b-b7ee-beec836ccbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load docker/entrypoint.sh\n",
    "#!/bin/bash\n",
    "uvicorn serving.app.main:app --host 0.0.0.0 --port 9090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5950c99-6bf1-4f8c-98e4-384f6b36abeb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load serving/app/main.py\n",
    "from asyncio import Queue\n",
    "\n",
    "from fastapi import FastAPI, Request\n",
    "from fastapi.responses import JSONResponse\n",
    "import asyncio\n",
    "import logging\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from model.dataset import clean_text\n",
    "\n",
    "app = FastAPI()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def start_model_server_loop():\n",
    "    \"\"\"\n",
    "    Start the model server loop upon application startup.\n",
    "\n",
    "    Roughly mimicking the Starlette setup from https://huggingface.co/docs/transformers/main/en/pipeline_webserver.\n",
    "    \"\"\"\n",
    "    q = asyncio.Queue()\n",
    "    app.state.model_queue = q\n",
    "    asyncio.create_task(model_server_loop(q))\n",
    "\n",
    "\n",
    "@app.post(\"/\")\n",
    "@app.post(\"/evaluate_text_sentiment\")\n",
    "async def evaluate_text_sentiment(request: Request):\n",
    "    \"\"\"\n",
    "    Evaluate the sentiment of the provided text. This endpoint accepts text data as raw payload, processes it through\n",
    "    the sentiment analysis model, and returns the sentiment.\n",
    "    \"\"\"\n",
    "    payload = await request.body()\n",
    "    string = payload.decode(\"utf-8\")\n",
    "    response_q = asyncio.Queue()\n",
    "    await request.app.state.model_queue.put((string, response_q))\n",
    "    return await response_q.get()\n",
    "\n",
    "\n",
    "async def model_server_loop(in_queue: Queue):\n",
    "    \"\"\"\n",
    "    A simple server loop, so requests can be processed sequentially by the model.\n",
    "    \"\"\"\n",
    "    model = pipeline(\n",
    "        task=\"sentiment-analysis\", model=\"christian-git-md/distilbert-base-uncased-finetuned-twitter-noleak\"\n",
    "    )\n",
    "    while True:\n",
    "        string, response_queue = await in_queue.get()\n",
    "        try:\n",
    "            string = clean_text(string)\n",
    "            out = model(string)\n",
    "            await response_queue.put(JSONResponse(content=out))\n",
    "        except (KeyboardInterrupt, SystemExit):\n",
    "            exit()\n",
    "        except Exception:\n",
    "            logger.exception(\"An error occured during model processing.\")\n",
    "            await response_queue.put(JSONResponse(status_code=500, content={\"error\": \"Internal server error\"}))\n",
    "\n",
    "\n",
    "# uvicorn serving.app.main:app --host 0.0.0.0 --port 9090\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edbc14ee-1152-4f46-aa96-21b176aac8d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"label\":\"Positive\",\"score\":0.9768452644348145}]"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://34.32.61.134:9090/ -d \"I love donuts\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cb3071-e14f-463b-b4db-b315caa44d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ich habe das distilbert-base-uncased-finetuned-twitter-noleak Modell auf einer vm bereitgestellt. Wir k√∂nnen Sentiment per POST Request beurteilen lassen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e176dd-efb9-4cce-a251-224041556991",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interpretation / Diskussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894c1a95-d4d4-4e8e-a899-d599c3548a17",
   "metadata": {
    "tags": []
   },
   "source": [
    "Gibt es am Mittwoch :)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
